{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveenKumarTV/NitIntern_nlp/blob/main/ImageTranslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrIiJCA-LsQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eaLIXPaKoUie",
        "outputId": "78802569-c001-4702-9442-79b88ccf6ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCuA6-6Y332J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40742b8e-b89c-4cb6-d951-251932596de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 234ms/step - accuracy: 0.1082 - loss: 3.2355 - val_accuracy: 0.2462 - val_loss: 2.8469\n",
            "Epoch 2/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2342 - loss: 2.7652 - val_accuracy: 0.4474 - val_loss: 2.0278\n",
            "Epoch 3/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4490 - loss: 1.9511 - val_accuracy: 0.6126 - val_loss: 1.4998\n",
            "Epoch 4/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5953 - loss: 1.4295 - val_accuracy: 0.6967 - val_loss: 1.0782\n",
            "Epoch 5/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7055 - loss: 1.0144 - val_accuracy: 0.7658 - val_loss: 0.9158\n",
            "Epoch 6/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7660 - loss: 0.7891 - val_accuracy: 0.8168 - val_loss: 0.7247\n",
            "Epoch 7/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8136 - loss: 0.6497 - val_accuracy: 0.8018 - val_loss: 0.7057\n",
            "Epoch 8/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8470 - loss: 0.4791 - val_accuracy: 0.7988 - val_loss: 0.6630\n",
            "Epoch 9/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8914 - loss: 0.3656 - val_accuracy: 0.8649 - val_loss: 0.5643\n",
            "Epoch 10/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9140 - loss: 0.2742 - val_accuracy: 0.8348 - val_loss: 0.5857\n",
            "Epoch 11/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9351 - loss: 0.2275 - val_accuracy: 0.8619 - val_loss: 0.5042\n",
            "Epoch 12/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9364 - loss: 0.2099 - val_accuracy: 0.8559 - val_loss: 0.5441\n",
            "Epoch 13/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9436 - loss: 0.1867 - val_accuracy: 0.8709 - val_loss: 0.4975\n",
            "Epoch 14/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9523 - loss: 0.1442 - val_accuracy: 0.8649 - val_loss: 0.4835\n",
            "Epoch 15/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9639 - loss: 0.1227 - val_accuracy: 0.8679 - val_loss: 0.4959\n",
            "Epoch 16/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.1191 - val_accuracy: 0.8859 - val_loss: 0.4934\n",
            "Epoch 17/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9657 - loss: 0.1121 - val_accuracy: 0.8709 - val_loss: 0.5191\n",
            "Epoch 18/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.1238 - val_accuracy: 0.8589 - val_loss: 0.5187\n",
            "Epoch 19/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.1023 - val_accuracy: 0.8949 - val_loss: 0.5156\n",
            "Epoch 20/20\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0872 - val_accuracy: 0.8859 - val_loss: 0.5218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "img_size = (64, 64)\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/TAMIL NLP/datasets/brami'\n",
        "num_classes = 27\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "def load_data():\n",
        "    X, y = [], []\n",
        "    for label in range(num_classes):\n",
        "        folder_path = os.path.join(data_path, str(label))\n",
        "        for file in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, file)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, img_size)\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    X = np.array(X, dtype=np.float32) / 255.0\n",
        "    X = X.reshape(-1, img_size[0], img_size[1], 1)\n",
        "    y = to_categorical(y, num_classes)\n",
        "    return X, y\n",
        "\n",
        "X, y = load_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(img_size[0], img_size[1], 1)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax', dtype='float32')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_ds, epochs=epochs, validation_data=test_ds)\n",
        "\n",
        "model.save(\"brahmi_to_tamil_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load trained model\n",
        "model = load_model(\"brahmi_to_tamil_model.h5\")\n",
        "\n",
        "# Image configuration\n",
        "img_size = (64, 64)\n",
        "\n",
        "# Label map (27 characters)\n",
        "label_map = {\n",
        "    0: 'அ', 1: 'ஆ', 2: 'இ', 3: 'ஈ', 4: 'உ', 5: 'ஊ', 6: 'எ', 7: 'ஏ', 8: 'ஐ',\n",
        "    9: 'ஒ', 10: 'ஓ', 11: 'ஔ', 12: 'க', 13: 'ங', 14: 'ச', 15: 'ஞ',\n",
        "    16: 'ட', 17: 'ண', 18: 'த', 19: 'ந', 20: 'ப', 21: 'ம',\n",
        "    22: 'ய', 23: 'ர', 24: 'ல', 25: 'வ', 26: 'ழ'\n",
        "}\n",
        "\n",
        "# Preprocess input word image (noise removal and thresholding)\n",
        "def preprocess_image(image):\n",
        "    # Invert and binarize\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to remove small noise\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    return clean\n",
        "\n",
        "# Segment characters using contours\n",
        "def segment_characters(image):\n",
        "    processed = preprocess_image(image)\n",
        "    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Extract and sort bounding boxes left-to-right\n",
        "    char_regions = [cv2.boundingRect(c) for c in contours]\n",
        "    char_regions = sorted(char_regions, key=lambda box: box[0])  # Sort by x-coordinate\n",
        "\n",
        "    characters = []\n",
        "    for (x, y, w, h) in char_regions:\n",
        "        # Filter out too-small boxes\n",
        "        if w < 10 or h < 10:\n",
        "            continue\n",
        "        char_img = image[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize and pad to 64x64\n",
        "        char_img = resize_and_pad(char_img)\n",
        "        characters.append(char_img)\n",
        "\n",
        "    return characters\n",
        "\n",
        "# Resize image and pad to make it square (64x64)\n",
        "def resize_and_pad(img):\n",
        "    h, w = img.shape\n",
        "    scale = 60.0 / max(h, w)\n",
        "    img = cv2.resize(img, (int(w * scale), int(h * scale)))\n",
        "    h, w = img.shape\n",
        "\n",
        "    # Padding to center the image\n",
        "    top = (64 - h) // 2\n",
        "    bottom = 64 - h - top\n",
        "    left = (64 - w) // 2\n",
        "    right = 64 - w - left\n",
        "\n",
        "    padded = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
        "    return padded\n",
        "\n",
        "# Predict Tamil word from Brahmi image\n",
        "def predict_word(image_path):\n",
        "    word = \"\"\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if image is None:\n",
        "        print(\"Error loading image.\")\n",
        "        return\n",
        "\n",
        "    char_imgs = segment_characters(image)\n",
        "\n",
        "    if not char_imgs:\n",
        "        print(\"No characters found.\")\n",
        "        return\n",
        "\n",
        "    for char_img in char_imgs:\n",
        "        norm = char_img / 255.0\n",
        "        input_img = norm.reshape(1, 64, 64, 1)\n",
        "        pred = model.predict(input_img)\n",
        "        label = np.argmax(pred)\n",
        "        word += label_map.get(label, \"?\")\n",
        "\n",
        "    print(\"Predicted Tamil Word:\", word)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"Input Brahmi Word: {word}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "predict_word(\"/content/drive/MyDrive/Colab Notebooks/TAMIL NLP/datasets/example_word3.png\")\n"
      ],
      "metadata": {
        "id": "84qr2KKeLz9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "da46a2ed-9ea9-4138-d60e-dff1bdde1370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'brahmi_to_tamil_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a62427167326>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brahmi_to_tamil_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Image configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'brahmi_to_tamil_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    }
  ]
}